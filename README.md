# Vision Weaver: A SupResDiffGAN Implementation

<div align="center">
  <p><strong>Fusing Latent Diffusion and GANs for High-Fidelity Super-Resolution</strong></p>
</div>

<div align="center">
  <a href="https://www.python.org/"><img src="https://img.shields.io/badge/Python-3.9%2B-blue?logo=python" alt="Python"></a>
  <a href="https://pytorch.org/"><img src="https://img.shields.io/badge/PyTorch-2.2%2B-ee4c2c?logo=pytorch" alt="PyTorch"></a>
  <a href="https://lightning.ai/docs/pytorch/stable/"><img src="https://img.shields.io/badge/PyTorch%20Lightning-2.2%2B-792ee5?logo=pytorch-lightning" alt="PyTorch Lightning"></a>
  <a href="https://hydra.cc/"><img src="https://img.shields.io/badge/Config-Hydra-89b8cd" alt="Hydra"></a>
  <a href="https://wandb.ai/"><img src="https://img.shields.io/badge/Logged-W%26B-yellowgreen" alt="Weights & Biases"></a>
</div>

**Vision Weaver** is a super-resolution project implementing a hybrid architecture inspired by **SupResDiffGAN**, combining **Denoising Diffusion Models** and **Generative Adversarial Networks (GANs)**. By performing diffusion in the **latent space** of a pre-trained tiny autoencoder, the model achieves a balance between **perceptual quality** (from diffusion) and **inference efficiency** (closer to GANs).

This implementation uses:
- **PyTorch** & **PyTorch Lightning**
- **Hydra** for configuration
- **Weights & Biases** for experiment tracking

---

<!--## Results Showcase

*(Placeholder: Replace with your best visual results after evaluation)*

<div align="center">
  <img src="https://via.placeholder.com/800x300.png?text=Sample+Super-Resolution+Results" alt="Sample Results" />
  <p><i>↑ LR (Left) | SR (Right) ↑</i></p>
</div>
-->

## Table of Contents

- [Key Features](#-key-features)
- [Evaluation Results](#-evaluation-results)
- [Getting Started](#️-getting-started)
  - [Prerequisites](#prerequisites)
  - [Installation](#installation)
- [Datasets](#-datasets)
- [Usage](#️-usage)
  - [Training / Fine-tuning](#training--fine-tuning)
  - [Evaluation](#evaluation)
- [Configuration](#️-configuration)
- [Model Architecture](#-model-architecture)
- [Acknowledgements](#-acknowledgements)
- [License](#-license)

---

## Key Features

- **Hybrid Generative Model**: Combines U-Net diffusion generator + patch-based adversarial discriminator.
- **Latent Space Diffusion**: Efficient inference via compressed latent space using `AutoencoderTiny`.
- **Flexible Configuration**: Hydra-powered `.yaml` configs in `conf/`.
- **Experiment Tracking**: Full integration with **Weights & Biases** (metrics, images, charts).
- **Modular Design**: Clean separation of model, data, and training logic using PyTorch Lightning.
- **Adaptive Noise Scheduling**: EMA-based noise step adaptation for stable GAN training.

---

## Evaluation Results

*Evaluated on the **CelebA-HQ test set** with the checkpoint `epoch=204`.*

| Posterior | Steps | PSNR ↑ | SSIM ↑ | LPIPS ↓ | MSE ↓          | Time (s/batch) ↓ |
|-----------|-------|--------|--------|---------|----------------|------------------|
| DDPM      | 50    | **26.227** | **0.750** | **0.1452** | **0.002560** | **1.28** |
| DDPM      | 100   | 26.136 | 0.748 | 0.1457 | 0.002613 | 2.45 |
| DDPM      | 200   | 26.055 | 0.746 | 0.1465 | 0.002661 | 5.44 |
| DDIM      | 50    | 26.236 | 0.748 | 0.1453 | 0.002555 | 1.52 |
| DDIM      | 100   | 26.205 | 0.748 | **0.1452** | 0.002574 | 2.49 |
| DDIM      | 200   | 26.176 | 0.747 | **0.1452** | 0.002590 | 4.73 |

> **Source:** `evaluation/final_evaluation.csv` (generated by `evaluate_model.py`).  
> Numbers are rounded to 4 decimal places for readability; bold highlights the best per-column value.

---

## Getting Started

### Prerequisites

- **Python**: `3.9+` (tested on `3.10`)
- **Conda** (recommended)
- **NVIDIA GPU** with CUDA
- **Kaggle Account & API Token** → place `kaggle.json` in:
  - Linux/WSL: `~/.kaggle/`
  - Windows: `C:\Users\<Your-Username>\.kaggle\`

### Installation

```bash
# 1. Clone the repository
git clone https://github.com/samarthya04/Super-Resolution-Diff-GAN.git
cd Super-Resolution-Diff-GAN

# 2. Create and activate conda environment
conda create -n SR_env python=3.10
conda activate SR_env

# 3. Install dependencies
pip install -r requirements-data.txt
pip install -r requirements-gpu.txt

# 4. Login to Weights & Biases
wandb login
```

> Ensure your CUDA version matches the PyTorch build in `requirements-gpu.txt`.

---

## Datasets

This project uses **CelebA-HQ**. A script automates download and preprocessing.

```bash
bash get_data.sh -c
```

This will:
- Download CelebA-HQ via Kaggle
- Unzip and split into `train`/`val`/`test`
- Generate **LR images** via bicubic downsampling
- Save under `data/celeb/`

---

## Usage

### Training / Fine-tuning

```bash
python train_model.py -cn config_supresdiffgan
```

#### To **resume** or **fine-tune**:

1. Edit `conf/config_supresdiffgan.yaml`:
   ```yaml
   mode: 'train-test'
   model:
     load_model: null  # or path to .ckpt
   trainer:
     resume_from_checkpoint: "models/checkpoints/your-best.ckpt"
   ```
2. Lower `model.lr` (e.g., `1e-5`) and adjust `trainer.max_epochs`.
3. Run:
   ```bash
   python train_model.py -cn config_supresdiffgan
   ```

> Checkpoints saved in `models/checkpoints/` (best + last).

---

### Evaluation

```bash
python evaluate_model.py -cn config_supresdiffgan
```

#### Before running:
1. Open `conf/config_supresdiffgan.yaml`
2. Set:
   ```yaml
   model:
     load_model: "models/checkpoints/<your best checkpoint>.ckpt"
   evaluation:
     steps: [50, 100, 200]
     posteriors: ["DDPM", "DDIM"]
     results_file: "evaluation/final_evaluation.csv"
   ```

> Outputs: Console, CSV, W&B bar charts.

---

## Configuration

All settings are in `conf/` via **Hydra**.

| File | Purpose |
|------|--------|
| `config_supresdiffgan.yaml` | Main training config |

**Override example**:
```bash
python train_model.py -cn config_supresdiffgan dataset.batch_size=16 model.lr=1e-4
```

---

## Model Architecture

| Component | Description |
|--------|-------------|
| **Autoencoder** | `AutoencoderTiny` from `diffusers` → latent compression |
| **U-Net Generator** | `UNet2DModel` with custom channels, ResNet, attention |
| **Discriminator** | Patch-based CNN with `BCEWithLogitsLoss` |
| **Perceptual Loss** | Optional VGG19 feature extractor |

See:
- `SupResDiffGAN/modules/UNet.py`
- `SupResDiffGAN/modules/Discriminator.py`
- `SupResDiffGAN/modules/FeatureExtractor.py`

---

## Acknowledgements

This project is inspired by and builds upon:
- [`dawir7/supresdiffgan`](https://github.com/dawir7/supresdiffgan) – original hybrid SR architecture
- `diffusers` library by Hugging Face
- Real-ESRGAN, ResShift, I2SB components

We thank the authors for open-sourcing their work.

---

## License

This project is currently **unlicensed**.  
Please add a `LICENSE` file (e.g., MIT, Apache 2.0) to clarify usage.

> Note: Adapted code may be subject to original licenses (BSD, NVIDIA, S-Lab, AFL 3.0).

---
